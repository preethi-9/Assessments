# -*- coding: utf-8 -*-
"""OnSpotQuiz_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fVYoiedCi0eBSmMlTP9eRDRTJyjtyx3r
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

data = pd.read_csv("/content/drive/MyDrive/DSAI-LVA-DATASET for Quiz_1.csv")
data.head()

import random
random.seed(0)
data = data.drop('ParentEducation',axis = 1)
selected_values = ['Masters','Bachelors','High School','School','Not Educated']
random_values = [random.choice(selected_values) for i in range(len(data))]
data['ParentEducation'] = random_values
data.head()

def pass_condition(PreviousTestScore):
    if PreviousTestScore >= 90 and PreviousTestScore <= 100:
        return "Pass With High Grade"
    elif PreviousTestScore >=56  and PreviousTestScore <= 89:
        return "Pass With Low Grade"
    else:
        return "Fail"
data['Pass'] = data['PreviousTestScore'].apply(pass_condition)

data

data.info()

data.describe()

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
encoder = LabelEncoder()
data['ParentEducation']=encoder.fit_transform(data['ParentEducation'])
data['Pass']=encoder.fit_transform(data['Pass'])
data.head()

import seaborn as sns
import matplotlib.pyplot as plt
corr_matrix = data.corr()
sns.heatmap(corr_matrix, annot=True, cmap='viridis')
plt.title('Heatmap of Students Data')
plt.show()

for i in data.columns:
  sns.histplot(data[i], kde=True)
  plt.title(f"Histplot of {i}")
  plt.xlabel(i)
  plt.ylabel('Frequency')
  plt.show()

sns.countplot(x='Pass', data=data)
plt.title('Countplot of Target Variable')
plt.xlabel('Pass')
plt.ylabel('Count')
plt.show()

split_ratios = 0.8
split_index = int(len(data) * split_ratios)

# Split the DataFrame
data_part1 = data.iloc[:split_index]
data_part2 = data.iloc[split_index:]

# Write data to separate files
data_part1.to_csv('Train.csv', index=False)
data_part2.to_csv('Test.csv', index=False)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

train_data = pd.read_csv('/content/Train.csv')
test_data = pd.read_csv('/content/Test.csv')

X_train, y_train = train_data.drop('Pass', axis=1), train_data['Pass']
X_test, y_test = test_data.drop('Pass', axis=1), test_data['Pass']

lr = LogisticRegression()
lr.fit(X_train, y_train)

rf = RandomForestClassifier()
rf.fit(X_train, y_train)

linear_reg_pred = lr.predict(X_test)
rf_pred = rf.predict(X_test)

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)

acc_score_rf = accuracy_score(y_test,rf_pred)
print("Accuracy score of random forest: ",acc_score_rf)
acc_score = accuracy_score(y_test,linear_reg_pred)
print("Accuracy score of Logistic Regression: ",acc_score)
acc_score_knn = accuracy_score(y_test,knn_pred)
print("Accuracy score of knn: ",acc_score_knn)

rf_classifier = XGBClassifier()
rf_classifier.fit(X_train,y_train)
rf_predictions = rf_classifier.predict(X_test)
acc_score_xgb = accuracy_score(y_test,rf_predictions)
print("Accuracy score of random forest: ",acc_score_xgb)

import matplotlib.pyplot as plt
sns.barplot([acc_score,acc_score_knn,acc_score_rf,acc_score_xgb])
plt.ylabel('Accuracy')
plt.xlabel(['LR','KNN','RF','XGB'])
plt.show()