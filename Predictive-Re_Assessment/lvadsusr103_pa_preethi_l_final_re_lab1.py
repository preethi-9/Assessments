# -*- coding: utf-8 -*-
"""LVADSUSR103_PA_Preethi_L_FINAL_RE_lab1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_PfVe8YBPnMPDb8YSjuaa3xQ-wdDaFbI
"""

#1
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from xgboost import XGBClassifier
from xgboost import XGBRegressor
from sklearn.metrics import accuracy_score,f1_score,mean_absolute_error,mean_squared_error,r2_score,classification_report,confusion_matrix

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/Fare prediction.csv")
data

data.shape

data.info()

# checking for duplicates
dup_no = data.duplicated().sum()
print('Total number of duplicated records: ', dup_no)
# drop duplicates if exist
data1 = data.drop_duplicates()
print(data1)

#finding null values
data1.isnull().sum()

data1.head(10)

data

# outliers check
data_num = data.drop(['key','pickup_datetime'],axis =1)
Q1 = data_num.quantile(0.25)
Q3 = data_num.quantile(0.75)
#interquartile range
IQR = Q3 - Q1
threshold = 1.5
Outliers = (data_num < (Q1 - threshold * IQR)) | (data_num > (Q3 + threshold * IQR))
data = data_num[~Outliers.any(axis=1)]
print('Number of outliers removed is :', len(data_num) - len(data))

#visualising the target variable using histogram
plt.figure(figsize=(9, 6))
sns.histplot(data['fare_amount'], bins=30, kde=True)
plt.title('Distribution of Fare Amount')
plt.xlabel('Fare Amount')
plt.ylabel('Frequency')
plt.show()

# Visualizing the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

#univariate analysis
numeric_columns = ['fare_amount', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']
plt.figure(figsize=(12, 8))
for i, column in enumerate(numeric_columns):
    plt.subplot(2, 3, i + 1)
    sns.histplot(data[column], kde=True)
    plt.title(f'Distribution of {column}')
plt.tight_layout()
plt.show()

X = data[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']]
y = data['fare_amount']

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#standardization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train the model
reg_model = LinearRegression()
reg_model.fit(X_train_scaled, y_train)

model1 = XGBRegressor()
model1.fit(X_train_scaled, y_train)

# Make predictions
y_pred = reg_model.predict(X_test_scaled)
y_pred1 = model1.predict(X_test_scaled)

# Evaluate the model
print("Using Linear Regression: ")
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error value is :", mse)
r2 = r2_score(y_test, y_pred)
print("R squared Error is :", r2)
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error (MAE) is :", mae,"\n")

print("Using XGBREGRESSOR: ")
mse1 = mean_squared_error(y_test, y_pred1)
print("Mean Squared Error value is :", mse1)
r2_1 = r2_score(y_test, y_pred1)
print("R squared Error is :", r2_1)
mae1 = mean_absolute_error(y_test, y_pred1)
print("Mean Absolute Error (MAE) is :", mae1)