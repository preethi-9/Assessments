# -*- coding: utf-8 -*-
"""LVADSUSR103_PA_Preethi_L_FINAL_RE_lab3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UYViRtSFzeuy6cuMcIX6dxRwZ31b_lzX
"""

#3
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler,LabelEncoder
from sklearn.metrics import silhouette_score
from sklearn.impute import SimpleImputer
from sklearn.ensemble import IsolationForest
from sklearn.svm import SVC
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from xgboost import XGBClassifier
from xgboost import XGBRegressor
from sklearn.metrics import accuracy_score,f1_score,mean_absolute_error,mean_squared_error,r2_score,classification_report,confusion_matrix

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/customer_segmentation.csv")
data

# Check for missing values
print("Missing Values: ")
print(data.isnull().sum())

# Initializing the LabelEncoder
label_encoder = LabelEncoder()
#3 categorical columns in the customer dataset
categorical_columns = ['Education', 'Marital_Status', 'Dt_Customer']

for cols in categorical_columns:
    data[cols] = label_encoder.fit_transform(data[cols])
print(data)

# Replace missing values with median
imputer = SimpleImputer(strategy='median')
df_imp = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)

#removing outliers
outliers_1 = IsolationForest(contamination=0.05, random_state=42)
outliers = outliers_1.fit_predict(df_imp)
df_cleaned1 = df_imp[outliers == 1]
print(df_cleaned1.describe())

# Visualization
features_to_visualize = numeric_columns = ['Income', 'Recency', 'MntWines']
plt.figure(figsize=(12, 8))
for i, column in enumerate(numeric_columns):
    plt.subplot(2, 3, i + 1)
    sns.histplot(data[column], kde=True)
    plt.title(f'Distribution of {column}')
plt.tight_layout()
plt.show()

# Feature Scaling
scaler = StandardScaler()
X = scaler.fit_transform(df_cleaned1)
# Splitting into training and test sets
X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)
# Fit KMeans clustering model on training set
kmeans = KMeans(random_state=42)

# Initializing list to store inertia values
inertia_values = []
# Range of clusters to test
num_clusters_range = range(1, 11)
for num_clusters in num_clusters_range:
    kmeans.set_params(n_clusters=num_clusters)
    kmeans.fit(X_train)
    inertia_values.append(kmeans.inertia_)

# Plot the Elbow Method
plt.plot(num_clusters_range, inertia_values, marker='*', linestyle='-')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.xticks(num_clusters_range)
plt.show()

#evaluation
silhouette_average = silhouette_score(X_test, kmeans.predict(X_test))
print("Silhouette Score:", silhouette_average)

# If you have a separate test dataset, you can read it similarly to how you read the training data
# Or, you can use the existing dataset and split it into training and test sets
# Then, you can predict clusters on the test set using the trained model
test_clusters = kmeans.predict(X_test)

# Print the predicted clusters on the test set
print("Predicted Clusters on Test Set:")
print(test_clusters)