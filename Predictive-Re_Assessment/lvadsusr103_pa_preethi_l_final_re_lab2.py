# -*- coding: utf-8 -*-
"""LVADSUSR103_PA_Preethi_L_FINAL_RE_lab2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xE2XAFp3ilaEqrquFOEa_P04uNRQ5_gK
"""

#2
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler,LabelEncoder
from sklearn.metrics import silhouette_score
from sklearn.impute import SimpleImputer
from sklearn.ensemble import IsolationForest
from sklearn.svm import SVC
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from xgboost import XGBClassifier
from xgboost import XGBRegressor
from sklearn.metrics import accuracy_score,f1_score,mean_absolute_error,mean_squared_error,r2_score,classification_report,confusion_matrix

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/penguins_classification.csv")
data

numeric_cols = ['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g','year']
imputer = SimpleImputer(strategy='median')
data[numeric_cols] = imputer.fit_transform(data[numeric_cols])

#Converting the  Categorical Variables to Numerical using Label Encoding
# Initialize LabelEncoder
label_encoder = LabelEncoder()
categorical_columns = ['species', 'island']
for col in categorical_columns:
    data[col] = label_encoder.fit_transform(data[col])

#handling outliers
columns_for_outliers = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
threshold = 3
# Remove outliers
df_cleaned = data[np.all(np.abs((data[columns_for_outliers] - data[columns_for_outliers].mean()) / data[columns_for_outliers].std()) < threshold, axis=1)]

print(df_cleaned.describe())

#Visualization
sns.pairplot(df_cleaned)
plt.show()

# Correlation Analysis
corr_matrix = df_cleaned.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

X = df_cleaned.drop(['species'], axis=1)
y = df_cleaned['species']

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Training Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train_scaled, y_train)

y_pred = rf_classifier.predict(X_test_scaled)

# Classification Report
print(classification_report(y_test, y_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, cmap='viridis', fmt='g')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()